{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99bd436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce91c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "train_path = \"CASIA_faceAntisp/train_release\"\n",
    "test_path = \"CASIA_faceAntisp/test_release\"\n",
    "\n",
    "dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda126f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self , num_classes=10):\n",
    "    super(CNN, self).__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=2) ,\n",
    "      nn.BatchNorm2d(16),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.layer2 = nn.Sequential(\n",
    "      nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2) ,\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.layer3 = nn.Sequential(\n",
    "      nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=3) ,\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.layer4 = nn.Sequential(\n",
    "      nn.Conv2d(64, 128, kernel_size=11, stride=1, padding=5) ,\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.fc = nn.Linear(8192 , num_classes)\n",
    "    \n",
    "  def forward(self , x):\n",
    "    # print(\"Entro layer 1\")\n",
    "    out = self.layer1(x)\n",
    "    # print(\"Layer 1 Shape: \", out.shape)\n",
    "    out = self.layer2(out)\n",
    "    # print(\"Layer 2 Shape: \", out.shape)\n",
    "    # print(\"Paso layer 2\")\n",
    "    out = self.layer3(out)\n",
    "    # print(\"Layer 3 Shape: \", out.shape)\n",
    "    # print(\"Paso layer 3\")\n",
    "    out = self.layer4(out)\n",
    "    # print(\"Layer 4 Shape: \", out.shape)\n",
    "    # print(\"Paso layer 4\")\n",
    "    out = out.reshape(out.size(0) , -1)\n",
    "    # print(\"Reshape Shape: \", out.shape)\n",
    "    # print(\"Reshapeo\")\n",
    "    out = self.fc(out)\n",
    "    # print(\"FC Shape: \", out.shape)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b67974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_subject(path, samples_per_video):\n",
    "    real_videos = ['1.avi', '2.avi', 'HR_1.avi', 'HR_4.avi']\n",
    "    subject = []\n",
    "    target = []\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    for dir in os.listdir(os.path.join(path)):\n",
    "        cap = cv2.VideoCapture(os.path.join(path, dir))\n",
    "        resampling_rate = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) / samples_per_video)\n",
    "        count = 0\n",
    "        failed = False\n",
    "        while cap.isOpened():\n",
    "            success, img = cap.read()\n",
    "            if success and (failed or (count%resampling_rate == 0)):\n",
    "                faces = face_cascade.detectMultiScale(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), 1.1, 4)\n",
    "                if len(faces) != 1:\n",
    "                    failed = True\n",
    "                    continue\n",
    "                (x, y, w, h) = faces[0]\n",
    "                subject.append(torch.Tensor(cv2.resize(img[y:y+h,x:x+w], dsize=(dim,dim))))\n",
    "                target.append(1 if dir in real_videos else 0)\n",
    "                failed = False\n",
    "            else:\n",
    "                break\n",
    "            count += 1\n",
    "    return subject, target\n",
    "\n",
    "def read_training_files(path, samples_per_video=16):\n",
    "    features = []\n",
    "    targets = []\n",
    "    for person in tqdm(os.listdir(path)):\n",
    "        f, t = get_training_subject(os.path.join(path,person), samples_per_video)\n",
    "        features += f\n",
    "        targets += t\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4139d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:29<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_targets = read_training_files(train_path, samples_per_video=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332d10a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:48<00:00,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "test_features, test_targets = read_training_files(test_path, samples_per_video=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5985a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.stack(train_features)\n",
    "train_features = train_features.permute(0, 3, 1, 2)\n",
    "test_features = torch.stack(test_features)\n",
    "test_features = test_features.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac38727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 3, 128, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c50324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = torch.Tensor(train_targets)\n",
    "test_targets = torch.Tensor(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fb555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2525424",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0063b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, num_epochs):\n",
    "  # train the model\n",
    "  list_loss= []\n",
    "  avg_list_loss = []\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    images = train_features.to(device)\n",
    "    labels = train_targets.type(torch.LongTensor).to(device)\n",
    "    output = model(images)\n",
    "    # print(output)\n",
    "    loss   = loss_fn(output, labels)\n",
    "    # change the params\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    list_loss.append(loss.item())\n",
    "            \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                  .format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "    avg_list_loss.append(np.mean(list_loss))\n",
    "\n",
    "    list_loss = []\n",
    "    \n",
    "  print('Finished Training Trainset')\n",
    "  return avg_list_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c1b27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(num_classes).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae74773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8061\n",
      "Epoch [2/100], Loss: 6.4806\n",
      "Epoch [3/100], Loss: 1.8838\n",
      "Epoch [4/100], Loss: 1.0938\n",
      "Epoch [5/100], Loss: 0.9600\n",
      "Epoch [6/100], Loss: 1.0213\n",
      "Epoch [7/100], Loss: 0.9383\n",
      "Epoch [8/100], Loss: 0.6427\n",
      "Epoch [9/100], Loss: 0.5575\n",
      "Epoch [10/100], Loss: 0.5651\n",
      "Epoch [11/100], Loss: 0.5778\n",
      "Epoch [12/100], Loss: 0.5815\n",
      "Epoch [13/100], Loss: 0.5723\n",
      "Epoch [14/100], Loss: 0.5318\n",
      "Epoch [15/100], Loss: 0.4691\n",
      "Epoch [16/100], Loss: 0.4151\n",
      "Epoch [17/100], Loss: 0.3932\n",
      "Epoch [18/100], Loss: 0.4164\n",
      "Epoch [19/100], Loss: 0.4510\n",
      "Epoch [20/100], Loss: 0.4394\n",
      "Epoch [21/100], Loss: 0.3956\n",
      "Epoch [22/100], Loss: 0.3651\n",
      "Epoch [23/100], Loss: 0.3558\n",
      "Epoch [24/100], Loss: 0.3576\n",
      "Epoch [25/100], Loss: 0.3624\n",
      "Epoch [26/100], Loss: 0.3622\n",
      "Epoch [27/100], Loss: 0.3515\n",
      "Epoch [28/100], Loss: 0.3321\n",
      "Epoch [29/100], Loss: 0.3120\n",
      "Epoch [30/100], Loss: 0.2995\n",
      "Epoch [31/100], Loss: 0.2978\n",
      "Epoch [32/100], Loss: 0.3020\n",
      "Epoch [33/100], Loss: 0.3024\n",
      "Epoch [34/100], Loss: 0.2950\n",
      "Epoch [35/100], Loss: 0.2846\n",
      "Epoch [36/100], Loss: 0.2765\n",
      "Epoch [37/100], Loss: 0.2717\n",
      "Epoch [38/100], Loss: 0.2691\n",
      "Epoch [39/100], Loss: 0.2665\n",
      "Epoch [40/100], Loss: 0.2624\n",
      "Epoch [41/100], Loss: 0.2563\n",
      "Epoch [42/100], Loss: 0.2489\n",
      "Epoch [43/100], Loss: 0.2420\n",
      "Epoch [44/100], Loss: 0.2370\n",
      "Epoch [45/100], Loss: 0.2337\n",
      "Epoch [46/100], Loss: 0.2304\n",
      "Epoch [47/100], Loss: 0.2256\n",
      "Epoch [48/100], Loss: 0.2192\n",
      "Epoch [49/100], Loss: 0.2127\n",
      "Epoch [50/100], Loss: 0.2071\n",
      "Epoch [51/100], Loss: 0.2023\n",
      "Epoch [52/100], Loss: 0.1976\n",
      "Epoch [53/100], Loss: 0.1927\n",
      "Epoch [54/100], Loss: 0.1874\n",
      "Epoch [55/100], Loss: 0.1818\n",
      "Epoch [56/100], Loss: 0.1760\n",
      "Epoch [57/100], Loss: 0.1700\n",
      "Epoch [58/100], Loss: 0.1642\n",
      "Epoch [59/100], Loss: 0.1586\n",
      "Epoch [60/100], Loss: 0.1527\n",
      "Epoch [61/100], Loss: 0.1461\n",
      "Epoch [62/100], Loss: 0.1392\n",
      "Epoch [63/100], Loss: 0.1327\n",
      "Epoch [64/100], Loss: 0.1265\n",
      "Epoch [65/100], Loss: 0.1205\n",
      "Epoch [66/100], Loss: 0.1147\n",
      "Epoch [67/100], Loss: 0.1090\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.0976\n",
      "Epoch [70/100], Loss: 0.0923\n",
      "Epoch [71/100], Loss: 0.0870\n",
      "Epoch [72/100], Loss: 0.0818\n",
      "Epoch [73/100], Loss: 0.0768\n",
      "Epoch [74/100], Loss: 0.0720\n",
      "Epoch [75/100], Loss: 0.0673\n",
      "Epoch [76/100], Loss: 0.0628\n",
      "Epoch [77/100], Loss: 0.0585\n",
      "Epoch [78/100], Loss: 0.0544\n",
      "Epoch [79/100], Loss: 0.0505\n",
      "Epoch [80/100], Loss: 0.0466\n",
      "Epoch [81/100], Loss: 0.0429\n",
      "Epoch [82/100], Loss: 0.0393\n",
      "Epoch [83/100], Loss: 0.0358\n",
      "Epoch [84/100], Loss: 0.0327\n",
      "Epoch [85/100], Loss: 0.0300\n",
      "Epoch [86/100], Loss: 0.0277\n",
      "Epoch [87/100], Loss: 0.0256\n",
      "Epoch [88/100], Loss: 0.0236\n",
      "Epoch [89/100], Loss: 0.0218\n",
      "Epoch [90/100], Loss: 0.0201\n",
      "Epoch [91/100], Loss: 0.0185\n",
      "Epoch [92/100], Loss: 0.0171\n",
      "Epoch [93/100], Loss: 0.0157\n",
      "Epoch [94/100], Loss: 0.0143\n",
      "Epoch [95/100], Loss: 0.0131\n",
      "Epoch [96/100], Loss: 0.0119\n",
      "Epoch [97/100], Loss: 0.0109\n",
      "Epoch [98/100], Loss: 0.0100\n",
      "Epoch [99/100], Loss: 0.0092\n",
      "Epoch [100/100], Loss: 0.0085\n",
      "Finished Training Trainset\n"
     ]
    }
   ],
   "source": [
    "avg_list_loss = train(model, optimizer, loss_fn, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39dc0c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c35a900490>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the loss\n",
    "plt.plot(avg_list_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b354841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.903954802259887"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    images = test_features.to(device)\n",
    "    labels = test_targets.type(torch.LongTensor).to(device)\n",
    "    outputs = model(images)\n",
    "\n",
    "\n",
    "o = []\n",
    "for output in outputs:\n",
    "    o.append(0) if output[0] > output[1] else o.append(1)\n",
    "\n",
    "for i in range(len(o)):\n",
    "    if o[i] == labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86fb9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export model into pickle file\n",
    "torch.save(model, 'model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "fce1d45328fd8024ce515f0ae0f0a25e82c54d2a05344803a525f4d98aa6e669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
