{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99bd436",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce91c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "train_path = \"CASIA_faceAntisp/train_release\"\n",
    "test_path = \"CASIA_faceAntisp/test_release\"\n",
    "\n",
    "dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda126f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self , num_classes=10):\n",
    "    super(CNN, self).__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=2) ,\n",
    "      nn.BatchNorm2d(16),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.layer2 = nn.Sequential(\n",
    "      nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2) ,\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.layer3 = nn.Sequential(\n",
    "      nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=3) ,\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.layer4 = nn.Sequential(\n",
    "      nn.Conv2d(64, 128, kernel_size=11, stride=1, padding=5) ,\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.fc = nn.Linear(8192 , num_classes)\n",
    "    \n",
    "  def forward(self , x):\n",
    "    # print(\"Entro layer 1\")\n",
    "    out = self.layer1(x)\n",
    "    # print(\"Layer 1 Shape: \", out.shape)\n",
    "    out = self.layer2(out)\n",
    "    # print(\"Layer 2 Shape: \", out.shape)\n",
    "    # print(\"Paso layer 2\")\n",
    "    out = self.layer3(out)\n",
    "    # print(\"Layer 3 Shape: \", out.shape)\n",
    "    # print(\"Paso layer 3\")\n",
    "    out = self.layer4(out)\n",
    "    # print(\"Layer 4 Shape: \", out.shape)\n",
    "    # print(\"Paso layer 4\")\n",
    "    out = out.reshape(out.size(0) , -1)\n",
    "    # print(\"Reshape Shape: \", out.shape)\n",
    "    # print(\"Reshapeo\")\n",
    "    out = self.fc(out)\n",
    "    # print(\"FC Shape: \", out.shape)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b67974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_subject(path, samples_per_video):\n",
    "    real_videos = ['1.avi', '2.avi', 'HR_1.avi', 'HR_4.avi']\n",
    "    subject = []\n",
    "    target = []\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    for dir in os.listdir(os.path.join(path)):\n",
    "        cap = cv2.VideoCapture(os.path.join(path, dir))\n",
    "        resampling_rate = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) / samples_per_video)\n",
    "        count = 0\n",
    "        failed = False\n",
    "        while cap.isOpened():\n",
    "            success, img = cap.read()\n",
    "            if success and (failed or (count%resampling_rate == 0)):\n",
    "                faces = face_cascade.detectMultiScale(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), 1.1, 4)\n",
    "                if len(faces) != 1:\n",
    "                    failed = True\n",
    "                    continue\n",
    "                (x, y, w, h) = faces[0]\n",
    "                subject.append(torch.Tensor(cv2.resize(img[y:y+h,x:x+w], dsize=(dim,dim))))\n",
    "                target.append(1 if dir in real_videos else 0)\n",
    "                failed = False\n",
    "            else:\n",
    "                break\n",
    "            count += 1\n",
    "    return subject, target\n",
    "\n",
    "def read_training_files(path, samples_per_video=16):\n",
    "    features = []\n",
    "    targets = []\n",
    "    for person in tqdm(os.listdir(path)):\n",
    "        f, t = get_training_subject(os.path.join(path,person), samples_per_video)\n",
    "        features += f\n",
    "        targets += t\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4139d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:29<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_targets = read_training_files(train_path, samples_per_video=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332d10a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:46<00:00,  3.56s/it]\n"
     ]
    }
   ],
   "source": [
    "test_features, test_targets = read_training_files(test_path, samples_per_video=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5985a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.stack(train_features)\n",
    "train_features = train_features.permute(0, 3, 1, 2)\n",
    "test_features = torch.stack(test_features)\n",
    "test_features = test_features.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac38727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 3, 128, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c50324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = torch.Tensor(train_targets)\n",
    "test_targets = torch.Tensor(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26fb555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2525424",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0063b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, num_epochs):\n",
    "  # train the model\n",
    "  list_loss= []\n",
    "  avg_list_loss = []\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    images = train_features.to(device)\n",
    "    labels = train_targets.type(torch.LongTensor).to(device)\n",
    "    output = model(images)\n",
    "    # print(output)\n",
    "    loss   = loss_fn(output, labels)\n",
    "    # change the params\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    list_loss.append(loss.item())\n",
    "            \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                  .format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "    avg_list_loss.append(np.mean(list_loss))\n",
    "\n",
    "    list_loss = []\n",
    "    \n",
    "  print('Finished Training Trainset')\n",
    "  return avg_list_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c1b27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(num_classes).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ae74773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8627\n",
      "Epoch [2/100], Loss: 7.6478\n",
      "Epoch [3/100], Loss: 2.4623\n",
      "Epoch [4/100], Loss: 1.0813\n",
      "Epoch [5/100], Loss: 0.8562\n",
      "Epoch [6/100], Loss: 0.9109\n",
      "Epoch [7/100], Loss: 0.9943\n",
      "Epoch [8/100], Loss: 0.7582\n",
      "Epoch [9/100], Loss: 0.6028\n",
      "Epoch [10/100], Loss: 0.5704\n",
      "Epoch [11/100], Loss: 0.5647\n",
      "Epoch [12/100], Loss: 0.5524\n",
      "Epoch [13/100], Loss: 0.5249\n",
      "Epoch [14/100], Loss: 0.4811\n",
      "Epoch [15/100], Loss: 0.4297\n",
      "Epoch [16/100], Loss: 0.3900\n",
      "Epoch [17/100], Loss: 0.3824\n",
      "Epoch [18/100], Loss: 0.4059\n",
      "Epoch [19/100], Loss: 0.4263\n",
      "Epoch [20/100], Loss: 0.4130\n",
      "Epoch [21/100], Loss: 0.3751\n",
      "Epoch [22/100], Loss: 0.3506\n",
      "Epoch [23/100], Loss: 0.3489\n",
      "Epoch [24/100], Loss: 0.3537\n",
      "Epoch [25/100], Loss: 0.3501\n",
      "Epoch [26/100], Loss: 0.3345\n",
      "Epoch [27/100], Loss: 0.3126\n",
      "Epoch [28/100], Loss: 0.2936\n",
      "Epoch [29/100], Loss: 0.2836\n",
      "Epoch [30/100], Loss: 0.2800\n",
      "Epoch [31/100], Loss: 0.2736\n",
      "Epoch [32/100], Loss: 0.2595\n",
      "Epoch [33/100], Loss: 0.2414\n",
      "Epoch [34/100], Loss: 0.2273\n",
      "Epoch [35/100], Loss: 0.2211\n",
      "Epoch [36/100], Loss: 0.2194\n",
      "Epoch [37/100], Loss: 0.2146\n",
      "Epoch [38/100], Loss: 0.2038\n",
      "Epoch [39/100], Loss: 0.1917\n",
      "Epoch [40/100], Loss: 0.1843\n",
      "Epoch [41/100], Loss: 0.1812\n",
      "Epoch [42/100], Loss: 0.1778\n",
      "Epoch [43/100], Loss: 0.1707\n",
      "Epoch [44/100], Loss: 0.1616\n",
      "Epoch [45/100], Loss: 0.1538\n",
      "Epoch [46/100], Loss: 0.1496\n",
      "Epoch [47/100], Loss: 0.1473\n",
      "Epoch [48/100], Loss: 0.1420\n",
      "Epoch [49/100], Loss: 0.1336\n",
      "Epoch [50/100], Loss: 0.1277\n",
      "Epoch [51/100], Loss: 0.1251\n",
      "Epoch [52/100], Loss: 0.1213\n",
      "Epoch [53/100], Loss: 0.1145\n",
      "Epoch [54/100], Loss: 0.1092\n",
      "Epoch [55/100], Loss: 0.1066\n",
      "Epoch [56/100], Loss: 0.1009\n",
      "Epoch [57/100], Loss: 0.0950\n",
      "Epoch [58/100], Loss: 0.0924\n",
      "Epoch [59/100], Loss: 0.0883\n",
      "Epoch [60/100], Loss: 0.0827\n",
      "Epoch [61/100], Loss: 0.0799\n",
      "Epoch [62/100], Loss: 0.0758\n",
      "Epoch [63/100], Loss: 0.0711\n",
      "Epoch [64/100], Loss: 0.0687\n",
      "Epoch [65/100], Loss: 0.0640\n",
      "Epoch [66/100], Loss: 0.0604\n",
      "Epoch [67/100], Loss: 0.0571\n",
      "Epoch [68/100], Loss: 0.0526\n",
      "Epoch [69/100], Loss: 0.0502\n",
      "Epoch [70/100], Loss: 0.0460\n",
      "Epoch [71/100], Loss: 0.0434\n",
      "Epoch [72/100], Loss: 0.0397\n",
      "Epoch [73/100], Loss: 0.0368\n",
      "Epoch [74/100], Loss: 0.0337\n",
      "Epoch [75/100], Loss: 0.0310\n",
      "Epoch [76/100], Loss: 0.0283\n",
      "Epoch [77/100], Loss: 0.0257\n",
      "Epoch [78/100], Loss: 0.0235\n",
      "Epoch [79/100], Loss: 0.0211\n",
      "Epoch [80/100], Loss: 0.0192\n",
      "Epoch [81/100], Loss: 0.0171\n",
      "Epoch [82/100], Loss: 0.0155\n",
      "Epoch [83/100], Loss: 0.0137\n",
      "Epoch [84/100], Loss: 0.0125\n",
      "Epoch [85/100], Loss: 0.0110\n",
      "Epoch [86/100], Loss: 0.0101\n",
      "Epoch [87/100], Loss: 0.0089\n",
      "Epoch [88/100], Loss: 0.0081\n",
      "Epoch [89/100], Loss: 0.0073\n",
      "Epoch [90/100], Loss: 0.0065\n",
      "Epoch [91/100], Loss: 0.0060\n",
      "Epoch [92/100], Loss: 0.0053\n",
      "Epoch [93/100], Loss: 0.0049\n",
      "Epoch [94/100], Loss: 0.0045\n",
      "Epoch [95/100], Loss: 0.0040\n",
      "Epoch [96/100], Loss: 0.0037\n",
      "Epoch [97/100], Loss: 0.0035\n",
      "Epoch [98/100], Loss: 0.0032\n",
      "Epoch [99/100], Loss: 0.0029\n",
      "Epoch [100/100], Loss: 0.0027\n",
      "Finished Training Trainset\n"
     ]
    }
   ],
   "source": [
    "avg_list_loss = train(model, optimizer, loss_fn, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39dc0c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dc5ad20370>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD8CAYAAABekO4JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAbsklEQVR4nO3da3Bc533f8e//nN3F4sKLCEIUKYmiKCmyLpNILqLqYseOZDuynbFftJ2RO/LEnqSsM2kjp556rOaFJ9M37dhN7U7aTDiOL41deRxZrlU1sS6xXSdjWzEk0TIlirJE3UhKJCiKAgESwJ5z/n1xzi4WIEgsSC73Ec7vM8PB3rD6nyH122f/53meY+6OiIiEK+p1ASIicmoKahGRwCmoRUQCp6AWEQmcglpEJHAKahGRwHUU1Gb2R2b2lJntNLN7zKze7cJERCS3ZFCb2YXAHwKj7n4tEAN3dLswERHJddr6qAD9ZlYBBoD93StJRETaVZZ6gbvvM7PPAy8Dx4GH3P2hU/3O+vXrfcuWLWenQhGREnjssccOufvIYs8tGdRmdh7wYeBS4Ajw12Z2p7t/fcHrtgHbADZv3szY2NgZFy4iUhZm9tLJnuuk9fEe4AV3H3f3BnAfcPPCF7n7dncfdffRkZFFPxREROQ0dBLULwM3mtmAmRlwG7Cru2WJiEjTkkHt7o8C9wKPA78ofmd7l+sSEZHCkj1qAHf/LPDZLtciIiKL0MpEEZHAKahFRAKnoBYRCVywQf3j5w6xZ3yy12WIiPRcsEH97+99kr/4f3t6XYaISM8FG9QzScZ0kva6DBGRngs2qJMso5FmvS5DRKTngg3qNHVmE+91GSIiPRdsUDc0ohYRAQIO6jRzBbWICIEGtbvTSJ0kVetDRCTIoM6KfJ7ViFpEJMygbrY81PoQEQk0qNNiSK2gFhEJNKibvemGetQiIoEGdZaPpGcTjahFRAINarU+RESalgxqM7vSzHa0/Zkws092s6hmUDd/ioiU2ZKX4nL33cB1AGYWA/uA73SzqKQ560OtDxGRZbc+bgOed/eXulFMU3MkrXnUIiLLD+o7gHu6UUi7uVkfCmoRkY6D2sxqwIeAvz7J89vMbMzMxsbHx8+oqOasj8zn5lSLiJTVckbU7wced/cDiz3p7tvdfdTdR0dGRs6oqPY9PjSqFpGyW05Qf4Rz0PaA+bM91KcWkbLrKKjNbAB4L3Bfd8vJJW3hrJkfIlJ2S07PA3D3Y8Bwl2tpae9Lay61iJRdkCsTG+2tD42oRaTkggzqNGtrfahHLSIlF2RQN+bN+lDrQ0TKLcigbu9Ra0QtImUXZFC3h7Om54lI2QUZ1PNG1DqZKCIlF2RQJ5l61CIiTWEGdfvJxEwjahEptyCDet70PLU+RKTkggxqTc8TEZkTZFBrep6IyJwgg7q9L63peSJSdkEGdar9qEVEWoIM6obmUYuItAQZ1PM3ZdLJRBEptyCDOkkds/y25lGLSNmFGdSZU6/EADQSjahFpNw6vRTXWjO718yeMbNdZnZTN4tK0oxaJSKOTCcTRaT0OroUF/BF4Hvu/s/NrAYMdLEmksypREY1VlCLiCwZ1Ga2GvgN4GMA7j4LzHazqCR1KrFRjSPNoxaR0uuk9bEVGAe+YmZPmNmXzGywm0XlI+qIWhxpRC0ipddJUFeAtwN/7u7XA1PAZxa+yMy2mdmYmY2Nj4+fUVFJlrVG1DqZKCJl10lQ7wX2uvujxf17yYN7Hnff7u6j7j46MjJyRkUlmRNHRiU2Tc8TkdJbMqjd/TXgFTO7snjoNuDpbhaVpBnVVutDI2oRKbdOZ338W+AbxYyPPcDHu1dSvnteHBkQaQm5iJReR0Ht7juA0S7X0tJInWpsRJE2ZRIR6XREfU41R9QRpul5IlJ6QQZ1I82oxFHrtohImQW510darEzUyUQRkUCDupE5lTjSEnIREQJtfaRZRiWyYlMmjahFpNyCDOokbW7KpCXkIiJBtj6SrLkpk1ofIiJhBnWaUYmiYq8PBbWIlFuYQd3cj7oSMasetYiUXJhBXexHrW1ORURCDerMiSNNzxMRgUCDOs0yqrFR0YhaRCTMoE7SfK+ParEy0V19ahEprzCDOnOqcUQtttZ9EZGyCjSos9aIGrQxk4iUW6BB7VTbg1rXTRSREgsuqNPMcSef9VHJy9Oe1CJSZh3t9WFmLwJHgRRI3L1rV3tJiovZ5vOo8x61Wh8iUmbL2ZTpN939UNcqKSTFSsRKZFQi9ahFRIJrfTRneFTiudaHglpEyqzToHbgITN7zMy2dbOgpAjl/AovzdaHTiaKSHl12vq4xd33m9n5wMNm9oy7/6j9BUWAbwPYvHnzaReUtkbUmp4nIgIdjqjdfX/x8yDwHeCGRV6z3d1H3X10ZGTktAtqZHM9agW1iEgHQW1mg2a2qnkbeB+ws1sFpa2TiVErqGc1j1pESqyT1scG4Dtm1nz9/3L373WroEb79LyKpueJiCwZ1O6+B/i1c1ALMNej1hJyEZFccNPzGq1ZH5HmUYuIEGBQp20nE+daH+pRi0h5BRfUzVDW9DwRkVxwQT03oo4U1CIiBBjUrZWJbSNqXYlcRMosvKBu71G39qPWiFpEyivAoG6OqCOqmkctIhJgUGubUxGRecIL6nmbMuUjavWoRaTMwg3qyDDLwzrRiFpESiy8oG5bmQhQjSO1PkSk1MIL6ra9PqAZ1Gp9iEh5hRfURSg351BX40hXIReRUgsuqNNiel5zRF2LTfOoRaTUggvqRmtEnQd1RT1qESm54II6PaFHbepRi0ipBRfUzZOJ6lGLiOQ6Dmozi83sCTN7oJsFNafntXrUlUjzqEWk1JYzor4L2NWtQpraF7yApueJiHQU1GZ2EfBB4EvdLSfflCkuViVC3qNW60NEyqzTEfUXgE8DXU/MJPPWaBq0MlFEZMmgNrPfBg66+2NLvG6bmY2Z2dj4+PhpF5Sk84O6pqAWkZLrZER9C/AhM3sR+CZwq5l9feGL3H27u4+6++jIyMhpF5RmTiWeK6sSG41EPWoRKa8lg9rd73b3i9x9C3AH8H13v7NbBTXSTK0PEZE2wc2jzkfU81sfOpkoImVWWc6L3f2HwA+7UkmhkXpri1PIR9SJpueJSIkFOKLO5o2oqxVT60NESi24oG5k3lqVCFpCLiISXFCnqVNta31oep6IlF1wQd1cmdhU0e55IlJyAQa1t/aihrz1kWbe2v5URKRswgvq9MQeNaD2h4iUVnhBnWXzVibWituJRtQiUlLhBXW6cFOm/LaumygiZRVeUC+cnldR60NEyi3AoM5afWmY61FrLrWIlFV4QX3CycSi9aEpeiJSUuEF9SLT80CtDxEpr+CCOs2cOFqk9aGTiSJSUsEFdSPNqC64wkvzcRGRMgouqNNFNmUCzaMWkfIKLqgb6fxLcWketYiUXScXt62b2T+a2c/N7Ckz+5NuFpRmCy7FVdH0PBEpt06u8DID3Oruk2ZWBf7BzP7W3X/ajYKS9MRLcYGm54lIeS0Z1O7uwGRxt1r86VpqJtn8JeSV1jxqjahFpJw66lGbWWxmO4CDwMPu/mi3Clq4KZPmUYtI2XUU1O6euvt1wEXADWZ27cLXmNk2Mxszs7Hx8fHTLmjhiLqmedQiUnLLmvXh7kfIr0J++yLPbXf3UXcfHRkZOa1issxx54SrkIN61CJSXp3M+hgxs7XF7X7gPcAz3SimkeWj5nlXIS9uJ5lG1CJSTp3M+tgIfM3MYvJg/5a7P9CNYpqX21p0ep5aHyJSUp3M+ngSuP4c1NJqb8SLLiFX60NEyimolYnNEXX7ftTN0bVmfYhIWQUV1EkRxu0j6jgyzBTUIlJeYQV1a0Q9F9RmRjWOtIRcREorrKBu9ajnl1WLIxqJetQiUk5hBXUxBa99RN28r+l5IlJWgQX1ibM+ID+5qB61iJRVWEGdNudRzy+rGkfMqvUhIiUVVlA3VyYuGFHXKhpRi0h5BRbUxYh6QY+6EpmCWkRKK6ygPknro16NOTab9qIkEZGeCyuoF9mUCWB4qMYbx2Z7UZKISM+FFdTpiZsyAQwP9vH6pIJaRMopqKBu7Z4Xzy9r/VCNQ5Mz5FcFExEpl6CCunnC8IQR9VCNmSRjSn1qESmhoII6Pcmsj+HBPgAOHZ055zWJiPRaUEHdWOTCAZCPqAFen1JQi0j5BBXUaWvBy8IedTGi1glFESmhTq6ZeLGZ/cDMdpnZU2Z2V7eKWewKL9A2olZQi0gJdXLNxAT4lLs/bmargMfM7GF3f/psF3OyHvW6wWZQq/UhIuWz5Ija3V9198eL20eBXcCF3SgmSRdvffRVYlbVK7w+pRG1iJTPsnrUZraF/EK3j3ajmOQkJxMh71Mf0ohaREqo46A2syHg28An3X1ikee3mdmYmY2Nj4+fVjGtlYnxiUE9PFhTj1pESqmjoDazKnlIf8Pd71vsNe6+3d1H3X10ZGTktIqZG1GfWNbwUE3T80SklDqZ9WHAXwK73P1Pu1lMq0e92Ih6SPt9iEg5dTKivgX4KHCrme0o/nygG8Wcskc9WOPwsdnWzBARkbJYcnqeu/8DcGJydkGSZcSRkQ/i5xse6sMd3jg221oAIyJSBkGtTEwyP2GxS5MWvYhIWQUV1GnqVE8W1MXGTFr0IiJlE1RQn2pEPbIqH1Ef0qIXESmZwII6oxovXpJG1CJSVmEFdXryEfWa/ipxZOpRi0jphBXUmZ90RB1FxrpBLXoRkfIJK6jT7KQjasiXkWtPahEpm7CCOvNFVyU2rR/qU49aREonrKBOfdFViU35fh8aUYtIuYQV1JkvuiFT0/Cg9vsQkfIJLKizU7Y+hodqTM4kTDfSc1iViEhvBRXUaXbq1sf61tXINaoWkfIIKqgbabZk6wO06EVEyiWooE6XmPWhjZlEpIyCCurGKVYmAq3tTXXtRBEpk6CCOj3FykSYG1Fr0YuIlElQQd1YYmXiQK1CfzVWj1pESqWTayZ+2cwOmtnObheTj6hPfTEZLXoRkbLpZET9VeD2LtcBNPejPnVJw0N9HDw6fS7KEREJwpJB7e4/Ag6fg1ry/ahP0foAuP7itTy65zAvvT51LkoSEem5oHrUp9qPuun3330Zldj4rw8/e9LXPD8+ySNPHzjb5YmI9MRZC2oz22ZmY2Y2Nj4+flrvke+ed+qSNqyu87GbL+W7P9/PM69NzHvO3fnaj1/kA1/8e37vf46xc9+bp1WHiEhIzlpQu/t2dx9199GRkZHTeo8kzU65hLzp9991GUN9FT7/4O7WY/uOHOdjX/kZn73/KW7cOsya/ipfeOTko24RkbeKSq8LaLfUftRNawaqfOJdl/G5B3fzrbFX+Ome17l/x34qsfEfP3wNd954CX/2/ef4Lw8/y5N7j/CrF609B9WLiHRHJ9Pz7gF+AlxpZnvN7He7VcxS+1G3+/gtW1g/1Men732Sv/3Fa9x54yU88u/exUdv2oKZ8bFbtrB2oMoXHvllt8oVETknlhxRu/tHzkUhAP/hg1fxtgtWdfTagVqF/3bHdezc/yb/4p9czHmDtXnPr6pX+Vfv3MrnHtzNjleOcN3FGlWLyFuTuftZf9PR0VEfGxs76++7XJMzCe/4z9/nuovX8tWP39DrckRETsrMHnP30cWeC2p63tk21FfhX//GZfxw9zjf3bGv1+WIiJyWFR3UAL/3zkv59S3ncfd9v+C5g0d7XY6IyLKt+KCuxhF/9i/fzkAt5hNff5ypmaTXJYmILMuKD2rIF8l88Y7r2TM+yae//aR23xORt5Sg5lF30y2Xr+dT77uSzz24m//75Ktcs2k1N20dZnioj8G+mKG+CiOr+tiwus6G1XXW9Fd7XbKICFCioAb4g9+8nHdesZ4fPTvO3//yEF/7yYs00sVnvWxeN8DoJecxumUdt111PhtW189tsSIihRU9PW8p7s50I2NyJmFyJmH86AyvTUyz743j7HjlDR576Q0OTc4SGbzjihH+2dsv5LeuuYB6Ne516SKywpxqel6pRtQLmRn9tZj+WszIqj4uXT8473l357mDk3x3x36+88Q+7vrmDtYN1vjIDRfz0Ru3cMEajbJFpPtKPaJejixzfrLndb724xd5eNcBYjPefeUIH/zVjdx21QZW1+f3tNPM2TM+ydOvTvDsgaPsfm2So9MNNq6ps3FtP1vXD3Lr285nuLhgr4iUm0bUZ0EUGbdcvp5bLl/PK4eP8Vc/fYn/8/P9PLLrILU4YuvIIGv6q6zur3J4apan909wvJECEEfG1vX582MvvcGBX7xKI3Uig1/fso73XLWB6zev5ZpNa+ivqa0iIvNpRH0Gssx54pUjfG/nq7xw6BgT0w0mjjdYVa9w7YVruHbTGq7etJqtI4P0VeJ5v7frtQkefOoADz31Gs+8li/EiSPjknUD1CoRldgwjJkk5XgjZaaRUY0japWIvkrEprX9bF43wMXrBviVDUNctXE16zU6F3nLOtWIWkEdgAMT0zy5902e3HuE58cnaaROmjmZO/VKzEAtplaJSDJnJsk4Ppuw78g0ew8f42jbAp6RVX38yoYhLh8Z4vLzh9i4pp91QzWGB2sM9VXoq8b0VSKqS1ycQUTOPbU+ArdhdZ33Xl3nvVdvWNbvuTuHp2bZfeAoT++fYNerR3nu4FG+/fg+Jk+xArMaG0N9FQb7KgwP1ti0tp8L1/Zz8boBLl0/yNaRQTat6SfqcMtZEekuBfVbmJkxPNTHzUN93HzZ+tbj7s6BiRkOTExzeGqWQ5MzHJtNmUnyFsqxRsrUTMLR6YRDkzM8e+AoP9h9kOlG1nqPWhyxaW2dTWv7uWB1naF6HuwD1Zg4NiqRUYmifNZMNR/1Dw/VGBmqM7KqT712kbNIQb0CmRkXrKkva/qguzN+dIbnx6fYc2iSl18/xr4jx9l35DiPvnCYqdmEyemEJOusVbaqr8L5q/s4f1Wd8warrK5XWVWvUK/G1OKIatFrz3vuMYO1mFXFa9YOVFk7UGN1vYKZRvUiCmoB8nA/f3Wd81fXuemy4UVf4+400rx3nmROkmYcb6Qcn02Zmkk5NDXDoaMzjE/OcHBihoNHpzkwMcOzByaZON5gYroxb9S+lEpkrOmvtmbTrKpX6KtEc732KCKOjVocMdgXM1CrMNRXyX9nIP+9ZotnsBZTjaPij+kDQN5SOgpqM7sd+CIQA19y9//U1aokSGZGrTI/4JZ73RwvQn42yWikGTNJxkwjY2o2b8VMHG/w5vEGbxyb5fDULG8W99883mBqJuH1yYzpooWTZBlJmr/XsUZK2uFoH/LWTl81ol7N93lZVc9DfqBWKUK/GPkXo/9me2ewr8JALW59ANSrMdXY8hk5cf5+9WreEqrFkT4Q5KxYMqjNLAb+O/BeYC/wMzO7392f7nZxsvKYWSvYziZ3ZzbNmJxOiqDPp0pOziQcm02YmklppFlr5sxskjGTpHNbCEw3ODqd8Max4xybTTg2m7Y+TPI/y58dVYmMgWLlazPIq3FEvZp/K6hXY/qr+YdAHvj5tMxm4PfXYuqV/HazRVSrzE3RbH4o1IvHm98W+ir5Nw6dDF45OhlR3wA85+57AMzsm8CHAQW1BMOsCKihuCurPZN07iTs1EzzZ8J0ktJInUaah/90I2O6kc99b35ATDdSZouwn2mkzCT5a9483uDAm/lrjzdSkrT4hlB80zhTtWLefTU2KsUHRV9bsOfnCua+DTQ/BJofKM0PjUrxwdr8IKhEc+9Ziaz12ubjcfFY/tOIo/x1keXPxVF+Mrp5OzIjiiAuno8iI7b8cTNavxcZpf2G0klQXwi80nZ/L/BPu1OOSJgqccTqODphq4BuyYqR/7HZJA/uRtb6JjCbpq2W0XQj5dhs/m2hkTmNJGM2zR+fbmTzvhXMtrWaZpL8d6YbGRPHk9YHzUySt5Qaaf5ejeL2ctpK3WQGRh7e7UHeDPHm8wtv518u2h8DY/5rWu9ffBY0n89vt72mVUxbXcXz6wZqfOsTN5314+4kqBf7CDvhb83MtgHbADZv3nyGZYmUWxTNbRgWgizzVmgnxbeD5jmCZktpNsnIihPOSZqRZsVJ5ywjzSBt/nQnLd7L3VuPZdncQq80cxzIisczp/WcOzj57zn5/TSbe9w9b4U5zHss/6xpPt/22rbXNZMtf8zbbs89TttzrceKu6vq3Zmf0cm77gUubrt/EbB/4YvcfTuwHfKViWelOhEJQhQZfVFMn+aJ9UQnZ3R+BlxhZpeaWQ24A7i/u2WJiEjTkp+P7p6Y2b8BHiSfnvdld3+q65WJiAjQ4Txqd/8b4G+6XIuIiCxC26iJiAROQS0iEjgFtYhI4BTUIiKBU1CLiASuK5fiMrNx4KXT/PX1wKGzWM5bQRmPGcp53GU8ZijncS/3mC9x95HFnuhKUJ8JMxs72XXDVqoyHjOU87jLeMxQzuM+m8es1oeISOAU1CIigQsxqLf3uoAeKOMxQzmPu4zHDOU87rN2zMH1qEVEZL4QR9QiItImmKA2s9vNbLeZPWdmn+l1Pd1iZheb2Q/MbJeZPWVmdxWPrzOzh83sl8XP83pd69lmZrGZPWFmDxT3y3DMa83sXjN7pvg7v2mlH7eZ/VHxb3unmd1jZvWVeMxm9mUzO2hmO9seO+lxmtndRb7tNrPfWs5/K4igbruA7vuBq4GPmNnVva2qaxLgU+5+FXAj8AfFsX4G+Dt3vwL4u+L+SnMXsKvtfhmO+YvA99z9bcCvkR//ij1uM7sQ+ENg1N2vJd8a+Q5W5jF/Fbh9wWOLHmfx//gdwDXF7/yPIvc64+49/wPcBDzYdv9u4O5e13WOjv275Fd43w1sLB7bCOzudW1n+TgvKv7h3go8UDy20o95NfACxbmgtsdX7HEzd43VdeTbKD8AvG+lHjOwBdi51N/twkwj39//pk7/O0GMqFn8AroX9qiWc8bMtgDXA48CG9z9VYDi5/m9q6wrvgB8Gmi/vPZKP+atwDjwlaLl8yUzG2QFH7e77wM+D7wMvAq86e4PsYKPeYGTHecZZVwoQd3RBXRXEjMbAr4NfNLdJ3pdTzeZ2W8DB939sV7Xco5VgLcDf+7u1wNTrIyv/CdV9GQ/DFwKbAIGzezO3lYVhDPKuFCCuqML6K4UZlYlD+lvuPt9xcMHzGxj8fxG4GCv6uuCW4APmdmLwDeBW83s66zsY4b83/Ved3+0uH8veXCv5ON+D/CCu4+7ewO4D7iZlX3M7U52nGeUcaEEdWkuoGtmBvwlsMvd/7TtqfuB3ylu/w5573pFcPe73f0id99C/nf7fXe/kxV8zADu/hrwipldWTx0G/A0K/u4XwZuNLOB4t/6beQnUFfyMbc72XHeD9xhZn1mdilwBfCPHb9rr5vxbc31DwDPAs8Df9zrerp4nO8g/8rzJLCj+PMBYJj8ZNsvi5/rel1rl47/3cydTFzxxwxcB4wVf9//GzhvpR838CfAM8BO4K+AvpV4zMA95H34BvmI+XdPdZzAHxf5tht4/3L+W1qZKCISuFBaHyIichIKahGRwCmoRUQCp6AWEQmcglpEJHAKahGRwCmoRUQCp6AWEQnc/wfONraKXlrHewAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the loss\n",
    "plt.plot(avg_list_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b354841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9067796610169492"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    images = test_features.to(device)\n",
    "    labels = test_targets.type(torch.LongTensor).to(device)\n",
    "    outputs = model(images)\n",
    "\n",
    "\n",
    "o = []\n",
    "for output in outputs:\n",
    "    o.append(0) if output[0] > output[1] else o.append(1)\n",
    "\n",
    "for i in range(len(o)):\n",
    "    if o[i] == labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86fb9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export model into pickle file\n",
    "torch.save(model, 'model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "fce1d45328fd8024ce515f0ae0f0a25e82c54d2a05344803a525f4d98aa6e669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
