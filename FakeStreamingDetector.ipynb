{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99bd436",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce91c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "train_path = \"CASIA_faceAntisp/train_release\"\n",
    "test_path = \"../../AntiSpoofing/CASIA_faceAntisp/test_release\"\n",
    "\n",
    "dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda126f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self , num_classes=10):\n",
    "    super(CNN, self).__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=2) ,\n",
    "      nn.BatchNorm2d(16),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.layer2 = nn.Sequential(\n",
    "      nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2) ,\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.layer3 = nn.Sequential(\n",
    "      nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=3) ,\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.layer4 = nn.Sequential(\n",
    "      nn.Conv2d(64, 128, kernel_size=11, stride=1, padding=5) ,\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "      # nn.Dropout(0.25))\n",
    "    self.fc = nn.Linear(8192 , num_classes)\n",
    "    \n",
    "  def forward(self , x):\n",
    "    # print(\"Entro layer 1\")\n",
    "    out = self.layer1(x)\n",
    "    # print(\"Layer 1 Shape: \", out.shape)\n",
    "    out = self.layer2(out)\n",
    "    # print(\"Layer 2 Shape: \", out.shape)\n",
    "    # print(\"Paso layer 2\")\n",
    "    out = self.layer3(out)\n",
    "    # print(\"Layer 3 Shape: \", out.shape)\n",
    "    # print(\"Paso layer 3\")\n",
    "    out = self.layer4(out)\n",
    "    # print(\"Layer 4 Shape: \", out.shape)\n",
    "    # print(\"Paso layer 4\")\n",
    "    out = out.reshape(out.size(0) , -1)\n",
    "    # print(\"Reshape Shape: \", out.shape)\n",
    "    # print(\"Reshapeo\")\n",
    "    out = self.fc(out)\n",
    "    # print(\"FC Shape: \", out.shape)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b67974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_subject(path, samples_per_video):\n",
    "    real_videos = ['1.avi', '2.avi', 'HR_1.avi', 'HR_4.avi']\n",
    "    subject = []\n",
    "    target = []\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    for dir in os.listdir(os.path.join(path)):\n",
    "        cap = cv2.VideoCapture(os.path.join(path, dir))\n",
    "        resampling_rate = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) / samples_per_video)\n",
    "        count = 0\n",
    "        failed = False\n",
    "        while cap.isOpened():\n",
    "            success, img = cap.read()\n",
    "            if success and (failed or (count%resampling_rate == 0)):\n",
    "                faces = face_cascade.detectMultiScale(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), 1.1, 4)\n",
    "                if len(faces) != 1:\n",
    "                    failed = True\n",
    "                    continue\n",
    "                (x, y, w, h) = faces[0]\n",
    "                subject.append(torch.Tensor(cv2.resize(img[y:y+h,x:x+w], dsize=(dim,dim))))\n",
    "                target.append(1 if dir in real_videos else 0)\n",
    "                failed = False\n",
    "            else:\n",
    "                break\n",
    "            count += 1\n",
    "    return subject, target\n",
    "\n",
    "def read_training_files(path, samples_per_video=16):\n",
    "    features = []\n",
    "    targets = []\n",
    "    for person in tqdm(os.listdir(path)):\n",
    "        f, t = get_training_subject(os.path.join(path,person), samples_per_video)\n",
    "        features += f\n",
    "        targets += t\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4139d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:25<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_targets = read_training_files(train_path, samples_per_video=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332d10a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:56<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "test_features, test_targets = read_training_files(test_path, samples_per_video=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5985a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.stack(train_features)\n",
    "train_features = train_features.permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac38727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 3, 128, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c50324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = torch.Tensor(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fb555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2525424",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0063b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, num_epochs):\n",
    "  # train the model\n",
    "  list_loss= []\n",
    "  avg_list_loss = []\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    images = train_features.to(device)\n",
    "    labels = train_targets.type(torch.LongTensor).to(device)\n",
    "    output = model(images)\n",
    "    # print(output)\n",
    "    loss   = loss_fn(output, labels)\n",
    "    # change the params\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    list_loss.append(loss.item())\n",
    "            \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                  .format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "    avg_list_loss.append(np.mean(list_loss))\n",
    "\n",
    "    list_loss = []\n",
    "    \n",
    "  print('Finished Training Trainset')\n",
    "  return avg_list_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c1b27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(num_classes).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae74773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8388\n",
      "Epoch [2/100], Loss: 5.2006\n",
      "Epoch [3/100], Loss: 1.2402\n",
      "Epoch [4/100], Loss: 0.9675\n",
      "Epoch [5/100], Loss: 0.7529\n",
      "Epoch [6/100], Loss: 0.7825\n",
      "Epoch [7/100], Loss: 0.7412\n",
      "Epoch [8/100], Loss: 0.6092\n",
      "Epoch [9/100], Loss: 0.5641\n",
      "Epoch [10/100], Loss: 0.5776\n",
      "Epoch [11/100], Loss: 0.5712\n",
      "Epoch [12/100], Loss: 0.5468\n",
      "Epoch [13/100], Loss: 0.5138\n",
      "Epoch [14/100], Loss: 0.4821\n",
      "Epoch [15/100], Loss: 0.4590\n",
      "Epoch [16/100], Loss: 0.4604\n",
      "Epoch [17/100], Loss: 0.4744\n",
      "Epoch [18/100], Loss: 0.4625\n",
      "Epoch [19/100], Loss: 0.4305\n",
      "Epoch [20/100], Loss: 0.4189\n",
      "Epoch [21/100], Loss: 0.4245\n",
      "Epoch [22/100], Loss: 0.4197\n",
      "Epoch [23/100], Loss: 0.4104\n",
      "Epoch [24/100], Loss: 0.4010\n",
      "Epoch [25/100], Loss: 0.3912\n",
      "Epoch [26/100], Loss: 0.3809\n",
      "Epoch [27/100], Loss: 0.3744\n",
      "Epoch [28/100], Loss: 0.3705\n",
      "Epoch [29/100], Loss: 0.3643\n",
      "Epoch [30/100], Loss: 0.3554\n",
      "Epoch [31/100], Loss: 0.3462\n",
      "Epoch [32/100], Loss: 0.3395\n",
      "Epoch [33/100], Loss: 0.3341\n",
      "Epoch [34/100], Loss: 0.3268\n",
      "Epoch [35/100], Loss: 0.3164\n",
      "Epoch [36/100], Loss: 0.3050\n",
      "Epoch [37/100], Loss: 0.2958\n",
      "Epoch [38/100], Loss: 0.2883\n",
      "Epoch [39/100], Loss: 0.2801\n",
      "Epoch [40/100], Loss: 0.2717\n",
      "Epoch [41/100], Loss: 0.2622\n",
      "Epoch [42/100], Loss: 0.2510\n",
      "Epoch [43/100], Loss: 0.2391\n",
      "Epoch [44/100], Loss: 0.2280\n",
      "Epoch [45/100], Loss: 0.2183\n",
      "Epoch [46/100], Loss: 0.2084\n",
      "Epoch [47/100], Loss: 0.1965\n",
      "Epoch [48/100], Loss: 0.1841\n",
      "Epoch [49/100], Loss: 0.1745\n",
      "Epoch [50/100], Loss: 0.1684\n",
      "Epoch [51/100], Loss: 0.1617\n",
      "Epoch [52/100], Loss: 0.1540\n",
      "Epoch [53/100], Loss: 0.1477\n",
      "Epoch [54/100], Loss: 0.1414\n",
      "Epoch [55/100], Loss: 0.1357\n",
      "Epoch [56/100], Loss: 0.1309\n",
      "Epoch [57/100], Loss: 0.1248\n",
      "Epoch [58/100], Loss: 0.1201\n",
      "Epoch [59/100], Loss: 0.1147\n",
      "Epoch [60/100], Loss: 0.1100\n",
      "Epoch [61/100], Loss: 0.1053\n",
      "Epoch [62/100], Loss: 0.1012\n",
      "Epoch [63/100], Loss: 0.0972\n",
      "Epoch [64/100], Loss: 0.0935\n",
      "Epoch [65/100], Loss: 0.0893\n",
      "Epoch [66/100], Loss: 0.0858\n",
      "Epoch [67/100], Loss: 0.0818\n",
      "Epoch [68/100], Loss: 0.0786\n",
      "Epoch [69/100], Loss: 0.0749\n",
      "Epoch [70/100], Loss: 0.0717\n",
      "Epoch [71/100], Loss: 0.0678\n",
      "Epoch [72/100], Loss: 0.0644\n",
      "Epoch [73/100], Loss: 0.0607\n",
      "Epoch [74/100], Loss: 0.0575\n",
      "Epoch [75/100], Loss: 0.0544\n",
      "Epoch [76/100], Loss: 0.0512\n",
      "Epoch [77/100], Loss: 0.0485\n",
      "Epoch [78/100], Loss: 0.0455\n",
      "Epoch [79/100], Loss: 0.0430\n",
      "Epoch [80/100], Loss: 0.0404\n",
      "Epoch [81/100], Loss: 0.0380\n",
      "Epoch [82/100], Loss: 0.0356\n",
      "Epoch [83/100], Loss: 0.0332\n",
      "Epoch [84/100], Loss: 0.0311\n",
      "Epoch [85/100], Loss: 0.0289\n",
      "Epoch [86/100], Loss: 0.0270\n",
      "Epoch [87/100], Loss: 0.0251\n",
      "Epoch [88/100], Loss: 0.0233\n",
      "Epoch [89/100], Loss: 0.0217\n",
      "Epoch [90/100], Loss: 0.0202\n",
      "Epoch [91/100], Loss: 0.0187\n",
      "Epoch [92/100], Loss: 0.0174\n",
      "Epoch [93/100], Loss: 0.0161\n",
      "Epoch [94/100], Loss: 0.0150\n",
      "Epoch [95/100], Loss: 0.0139\n",
      "Epoch [96/100], Loss: 0.0128\n",
      "Epoch [97/100], Loss: 0.0119\n",
      "Epoch [98/100], Loss: 0.0110\n",
      "Epoch [99/100], Loss: 0.0103\n",
      "Epoch [100/100], Loss: 0.0095\n",
      "Finished Training Trainset\n"
     ]
    }
   ],
   "source": [
    "avg_list_loss = train(model, optimizer, loss_fn, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39dc0c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x201806d6370>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the loss\n",
    "plt.plot(avg_list_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bigdata-2022-2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "66f09b424765b63542405763d1a10db6c28a98e754f61b0d319e8110760975c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
